{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier shape representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "# import open3d as o3d\n",
    "# from open3d import JVisualizer\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import trimesh\n",
    "from trimesh.sample import sample_surface\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build model & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeometryDataset(data.Dataset):\n",
    "    def __init__(self, mesh_path, samples, *args):\n",
    "        super(GeometryDataset, self).__init__(*args)\n",
    "        self.mesh = trimesh.load(mesh_path)\n",
    "#         self.mesh.vertices[:,1]+=-0.1\n",
    "        # self.mesh.vertices /= 100\n",
    "        self.sample = sample_surface(self.mesh, samples)\n",
    "        self.pnts = torch.from_numpy(self.sample[0])\n",
    "        self.normals = torch.from_numpy(self.mesh.face_normals[self.sample[1]])\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        points = self.pnts[index]\n",
    "        normals = self.normals[index]\n",
    "        return points, normals\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pnts)\n",
    "\n",
    "class NormalPerPoint(object):\n",
    "    def __init__(self, global_sigma, local_sigma=0.01):\n",
    "        self.global_sigma = global_sigma\n",
    "        self.local_sigma = local_sigma\n",
    "\n",
    "    def get_points(self, pc_input, local_sigma=None):\n",
    "        batch_size, dim = pc_input.shape\n",
    "\n",
    "        if local_sigma is not None:\n",
    "            sample_local = pc_input + (\n",
    "                torch.randn_like(pc_input) * local_sigma.unsqueeze(-1)\n",
    "            )\n",
    "        else:\n",
    "            sample_local = pc_input + (torch.randn_like(pc_input) * self.local_sigma)\n",
    "\n",
    "        sample_global = (\n",
    "            torch.rand(batch_size // 8, dim, device=pc_input.device)\n",
    "            * (self.global_sigma * 2)\n",
    "        ) - self.global_sigma\n",
    "\n",
    "        sample = torch.cat([sample_local, sample_global], dim=0)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class ImplicitNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_out,\n",
    "        dims,\n",
    "        geometric_init=True,\n",
    "        bias=1.0,\n",
    "        skip_in=(),\n",
    "        weight_norm=True,\n",
    "        multires=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dims = [d_in] + dims + [d_out]\n",
    "        self.embed_fn = None\n",
    "        if multires > 0:\n",
    "            embed_fn, input_ch = get_embedder(multires)\n",
    "            self.embed_fn = embed_fn\n",
    "            dims[0] = input_ch\n",
    "        self.num_layers = len(dims)\n",
    "        self.skip_in = skip_in\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            if l + 1 in self.skip_in:\n",
    "                out_dim = dims[l + 1] - dims[0]\n",
    "            else:\n",
    "                out_dim = dims[l + 1]\n",
    "            lin = nn.Linear(dims[l], out_dim)\n",
    "            if geometric_init:\n",
    "                if l == self.num_layers - 2:\n",
    "                    torch.nn.init.normal_(\n",
    "                        lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[l]), std=0.0001\n",
    "                    )\n",
    "                    torch.nn.init.constant_(lin.bias, -bias)\n",
    "                elif multires > 0 and l == 0:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.constant_(lin.weight[:, 3:], 0.0)\n",
    "                    torch.nn.init.normal_(\n",
    "                        lin.weight[:, :3], 0.0, np.sqrt(2) / np.sqrt(out_dim)\n",
    "                    )\n",
    "                elif multires > 0 and l in self.skip_in:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.normal_(\n",
    "                        lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim)\n",
    "                    )\n",
    "                    torch.nn.init.constant_(lin.weight[:, -(dims[0] - 3) :], 0.0)\n",
    "                else:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "                    torch.nn.init.normal_(\n",
    "                        lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim)\n",
    "                    )\n",
    "\n",
    "            if weight_norm:\n",
    "                lin = nn.utils.weight_norm(lin)\n",
    "\n",
    "            setattr(self, \"lin\" + str(l), lin)\n",
    "\n",
    "        self.softplus = nn.Softplus(beta=100)\n",
    "\n",
    "    def forward(self, input, compute_grad=False):\n",
    "        if self.embed_fn is not None:\n",
    "            input = self.embed_fn(input)\n",
    "        x = input\n",
    "        for l in range(0, self.num_layers - 1):\n",
    "            lin = getattr(self, \"lin\" + str(l))\n",
    "            if l in self.skip_in:\n",
    "                x = torch.cat([x, input], 1) / np.sqrt(2)\n",
    "            x = lin(x)\n",
    "            if l < self.num_layers - 2:\n",
    "                x = self.softplus(x)\n",
    "        return x\n",
    "\n",
    "    def gradient(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        y = self.forward(x)[:, :1]\n",
    "        d_output = torch.ones_like(y, requires_grad=False, device=y.device)\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=y,\n",
    "            inputs=x,\n",
    "            grad_outputs=d_output,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        return gradients.unsqueeze(1)\n",
    "\n",
    "from torch.autograd import grad\n",
    "def gradient(inputs, outputs):\n",
    "    d_points = torch.ones_like(outputs, requires_grad=False, device=outputs.device)\n",
    "    points_grad = grad(\n",
    "        outputs=outputs,\n",
    "        inputs=inputs,\n",
    "        grad_outputs=d_points,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0][:, -3:]\n",
    "    return points_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh_path = \"./bunny/reconstruction/bun_zipper.ply\"\n",
    "# dataset =GeometryDataset(mesh_path,samples=100000)\n",
    "# from torch.utils.data import DataLoader\n",
    "# dataloader = DataLoader(dataset,batch_size=1000, num_workers=4,pin_memory=True)\n",
    "\n",
    "# mesh=dataset.mesh\n",
    "# vertices = dataset.pnts\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# # ax.plot_trisurf(mesh.vertices[:, 0], mesh.vertices[:,1], triangles=mesh.faces, Z=mesh.vertices[:,2]) \n",
    "# plot_geeks = ax.scatter(vertices[:, 0], vertices[:,1], vertices[:,2],color='green')\n",
    "# ax.set_title(\"3D plot\")\n",
    "# ax.set_xlabel('x-axis')\n",
    "# ax.set_ylabel('y-axis')\n",
    "# ax.set_zlabel('z-axis')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "model = ImplicitNetwork(d_in=3,d_out=1,dims=[512, 512, 512, 512, 512, 512, 512, 512],bias=0.2,skip_in=[4]).cuda()\n",
    "sampler = NormalPerPoint(global_sigma=0.1, local_sigma=0.01)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.0005,\n",
    "    weight_decay=0,\n",
    ")\n",
    "model.load_state_dict(torch.load(\"./checkpoints/model_0072000.pth\")[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt_groups = {'mnfld_loss_train':[],'grad_loss':[]}\n",
    "# # plotlosses_model = PlotLosses(groups=plt_groups)\n",
    "# plotlosses_model = PlotLosses()\n",
    "# steps=0\n",
    "# for i in range(1000000):\n",
    "#     for mnfld_pnts, normals in tqdm(dataloader, leave=False):\n",
    "#         mnfld_pnts=mnfld_pnts.cuda()\n",
    "#         normals=normals.cuda()\n",
    "#         nonmnfld_pnts = sampler.get_points(mnfld_pnts)\n",
    "#         # forward\n",
    "#         mnfld_pnts.requires_grad_()\n",
    "#         nonmnfld_pnts.requires_grad_()\n",
    "#         mnfld_pred = model(mnfld_pnts)\n",
    "#         nonmnfld_pred = model(nonmnfld_pnts)\n",
    "#         mnfld_grad = gradient(mnfld_pnts, mnfld_pred)\n",
    "#         nonmnfld_grad = gradient(nonmnfld_pnts, nonmnfld_pred)\n",
    "#         # maniflod_loss\n",
    "#         mnfld_loss = (mnfld_pred.abs()).mean()\n",
    "#         # eikonal loss\n",
    "#         grad_loss = ((nonmnfld_grad.norm(2, dim=-1) - 1) ** 2).mean()\n",
    "#         # regularize: prevents off-surface locations to create zero-isosurface\n",
    "#         sdf_global_without_surface = nonmnfld_pred[mnfld_pnts.shape[0] :]\n",
    "#         reg_loss = torch.exp(\n",
    "#             -100 * (sdf_global_without_surface.abs())\n",
    "#         ).mean()\n",
    "\n",
    "#         loss = (\n",
    "#             mnfld_loss\n",
    "#             + 0.1 * grad_loss\n",
    "#             + 0.1 * reg_loss\n",
    "#         )\n",
    "#         # normal loss\n",
    "#         if False:\n",
    "#             normals = normals.view(-1, 3)\n",
    "#             normals_loss = ((mnfld_grad - normals).abs()).norm(2, dim=1).mean()\n",
    "#             loss = loss + cfg.SOLVER.LOSS.NORMAL_LAMBDA * normals_loss\n",
    "#         # back propagation\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         steps+=1\n",
    "#         if steps%100==0:\n",
    "#             torch.save(model.state_dict(), \"./checkpoints/last.pth\")\n",
    "#             plotlosses_model.update({'loss':loss.item(),'mnfld_loss_train':mnfld_loss.item(),'grad_loss':grad_loss.item()}, current_step=steps)\n",
    "#             plotlosses_model.send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get volume and mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "face_normals incorrect shape, ignoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of inner points : 192972\n",
      "number of outer points : 807028\n"
     ]
    }
   ],
   "source": [
    "from plot import plot_surface\n",
    "RESULOTION=100\n",
    "mesh,volume = plot_surface(model,path=\"./results\",iteration=72000, shapename=\"result\",resolution=RESULOTION,mc_value=0.,is_uniform_grid=True,verbose=False,\n",
    "                        save_html=False,\n",
    "                        save_ply=True,\n",
    "                        overwrite=True,cube_length=1.5)\n",
    "print(\"number of inner points :\",(volume<0).sum())\n",
    "print(\"number of outer points :\",(volume>0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier transform for geometry\n",
    "### visualize volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from skimage import measure\n",
    "def get_mesh(volume):\n",
    "    verts, faces, normals, values = measure.marching_cubes_lewiner(volume=volume,level=0.)\n",
    "    meshexport = trimesh.Trimesh(verts, faces, normals, vertex_colors=values)   \n",
    "    return meshexport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mesh=get_mesh(volume)\n",
    "# _=test_mesh.export(\"gt.ply\", \"ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT for shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F=torch.fft.fftn(torch.from_numpy(volume).cuda(),s=[100,100,100])\n",
    "F_shift=torch.fft.fftshift(F)\n",
    "F_low=torch.zeros_like(F_shift)\n",
    "F_low[40:60,40:60,40:60]=F_shift[40:60,40:60,40:60]\n",
    "F_high=F_shift.clone()\n",
    "F_high[40:60,40:60,40:60]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFFT for shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=torch.fft.ifftn(torch.fft.ifftshift(F_shift))\n",
    "f=torch.real(f).cpu().numpy()\n",
    "\n",
    "f_low=torch.fft.ifftn(torch.fft.ifftshift(F_low))\n",
    "f_low=torch.real(f_low).cpu().numpy()\n",
    "\n",
    "f_high=torch.fft.ifftn(torch.fft.ifftshift(F_high))\n",
    "f_high=torch.real(f_high).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mesh=get_mesh(f_low)\n",
    "_=res_mesh.export(\"low.ply\", \"ply\")\n",
    "res_mesh=get_mesh(f_high)\n",
    "_=res_mesh.export(\"high.ply\", \"ply\")\n",
    "res_mesh=get_mesh(f)\n",
    "_=res_mesh.export(\"full.ply\", \"ply\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Fourier transform for geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_3(fscale):\n",
    "    fx = torch.fft.fftfreq(int(100* fscale)) * fscale\n",
    "    x_train = torch.stack(torch.meshgrid(fx.reshape(-1), fx.reshape(-1),fx.reshape(-1)), -1)\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/p300/Anacondas/envs/ndface/lib/python3.7/site-packages/torch/functional.py:568: UserWarning:\n",
      "\n",
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100, 100, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.fft.fftfreq(int(100))\n",
    "freq=fft_3(1.0)\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F[99,99,99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kspace=ImplicitNetwork(d_in=3,d_out=2,dims=[512, 512, 512, 512],geometric_init=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 11.93 GiB total capacity; 9.57 GiB already allocated; 1.74 GiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-844945577afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_kspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/p300/Anacondas/envs/ndface/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-94720647fc18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, compute_grad)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p300/Anacondas/envs/ndface/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/p300/Anacondas/envs/ndface/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 11.93 GiB total capacity; 9.57 GiB already allocated; 1.74 GiB free; 9.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model_kspace(freq.cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ndface",
   "language": "python",
   "name": "ndface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
